# Efficient-Error-Correction

Public repository for data and scripts related to my master's thesis "Efficient Error Correction: Reducing the Workload for Optical Early-Music Recognition"

## Input data

The input data is located in the "Input Data" folder. This folder contains three subfolders:
1. res_dict_lc
2. source_images
3. lc_data_combined
The first subfolder holds the OMR output data. This means that it contains an individual pickle file for every folio of the *Leuven Chansonnier*. These pickle files are serialised detection dictionaries that represent the symbols detected by the OMR algorithm on that folio

The second subfolder holds the source images. These are individual images of every folio of the *Leuven Chansonnier*. The images were taken from IDEM (Integrated Database for Early Music), a database made available by the Alamire Foundation in Leuven.

The third folder combines the OMR dictionaries from folder 1 on a piece-by-piece basis. Every piece has its own subfolder with a "file_data.txt" file that contains its composer, associated pickle files and associated image files. It also contains a "combined_dict.pickle" file, a file that combines the separate dictionaries for the folios of the piece.

## Generated Files

The "Generated Files" folder holds all the files generated by the correction tool. Every piece has its own subfolder. Each of these subfolders contain a "raw.mei", the mei encoding of the OMR output, a "temp.mei" an encoding of the raw input where the dissonances have been visualised, and a set of svg files. These svg files hold the visualisation of "temp.mei"

Keep in mind that these are all raw outputs of the the OMR algorithm and that the scoring-up tool was not yet used to align the pieces with perfect mensuration.

## Output Files

The "Output Files" folder contains fifteen fully corrected MEI files. These files were corrected using the correction tool.

## Python Scripts

Five python scripts have been included, these are the scripts used for the experiments in chapter 5 of my thesis.

### average_piece.py

This script can be used to obtain the expected workload for using the correction tool without any efficient correction technique. It can differentiate between the four separate estimates and automatically generates a plot of the number of errors compared to the average number of staves to be checked.

### chance_of_consecutive_intervals.py

This script computes the probability that consecutive random three voice textures contain only consonant intervals. For determining whether or not an interval is dissonant, it uses the same logic employed by the correction tool's dissonance visualisation. Lastly, it provides a plot that compares the number of consecutive intervals to the probability that they are all consonant.

### comparison_plotter.py

This script takes the workloads for different error counts for all four correction strategies as it inputs and then creates comparative plots for them.

### correction_strategies.py

This script provides the likelihood that all errors will be found in a segment with a given amount of errors if only a subset of the error categories are examined. Additionally, it can create plots for this and the probability that individual subsets of errors are the most efficient. Furthermore, it can provide these same likelihoods for the alternative OC strategies employed by EE.

## error_elim_experiment.py

This script computes the probabilities for specific EE approaches to reduce the score to a given amount of errors. These values are needed to obtain their workload reduction, as the alternative OC strategy must then be used on these scores with reduced errors. 